{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "### please specify your input path here\n",
    "PROJECT_FOLDER = \"YOUR_PROJECT_FOLDER\" # parent folder of the input images\n",
    "IMAGE_DATA_FOLDER = PROJECT_FOLDER + \"images/\" # folder of the input images\n",
    "INPUT_TEST_CSV_FILE = \"YOUR_TEST_FILE\" # csv file list locations / paths to test cases (dicom)\n",
    "OUTPUT_FILE = \"YOUR_OUTPUT_FILE\" # output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a05898",
   "metadata": {
    "papermill": {
     "duration": 89.369973,
     "end_time": "2022-10-22T09:12:16.754051",
     "exception": false,
     "start_time": "2022-10-22T09:10:47.384078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q ./for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "# !pip install -q ./for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !pip install -q ./for-pydicom/pylibjpeg_libjpeg-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d596eca0-a7fc-4d82-bc53-0c08e3bd84fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601eebd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:47:38.950950Z",
     "start_time": "2023-06-21T16:47:38.947675Z"
    },
    "papermill": {
     "duration": 0.015111,
     "end_time": "2022-10-22T09:12:16.775638",
     "exception": false,
     "start_time": "2022-10-22T09:12:16.760527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./segmentation-models-pytorch/segmentation_models.pytorch-master\")\n",
    "sys.path.append('./timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append(\"./pretrainedmodels/pretrainedmodels-0.7.4\")\n",
    "sys.path.append(\"./efficientnet-pytorch/EfficientNet-PyTorch-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ab6700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:47:49.229574Z",
     "start_time": "2023-06-21T16:47:43.066044Z"
    },
    "papermill": {
     "duration": 9.448257,
     "end_time": "2022-10-22T09:12:26.229775",
     "exception": false,
     "start_time": "2022-10-22T09:12:16.781518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "import gc\n",
    "gc.enable()\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "if torch.cuda.is_available(): device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd93eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:51:13.163208Z",
     "start_time": "2023-06-21T16:51:13.154894Z"
    },
    "papermill": {
     "duration": 0.028958,
     "end_time": "2022-10-22T09:12:26.265150",
     "exception": false,
     "start_time": "2022-10-22T09:12:26.236192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.dicomDatasets import LoadDicoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea257f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:21.688979Z",
     "start_time": "2023-06-21T16:48:21.685250Z"
    },
    "papermill": {
     "duration": 0.019717,
     "end_time": "2022-10-22T09:12:26.345600",
     "exception": false,
     "start_time": "2022-10-22T09:12:26.325883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SagSegModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SagSegModel, self).__init__()\n",
    "        #tf_efficientnet_b0_ns resnest50d_4s2x40d seresnext50_32x4d tf_efficientnetv2_m_in21ft1k\n",
    "        self.feature_extractor = smp.Unet('tu-tf_efficientnet_b1_ns', in_channels=1, classes=8, encoder_weights=None)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        masks = self.feature_extractor(inp)\n",
    "        return masks\n",
    "    \n",
    "def SagInference(models, image, L):\n",
    "    with torch.no_grad():\n",
    "        img = cv2.resize(image, (256, 256)).astype(np.float32) / np.max(image)\n",
    "        \n",
    "        outputs = []\n",
    "        for model in models:\n",
    "            output = model.sigmoid(model(torch.as_tensor(img).unsqueeze(0).unsqueeze(0).to(device)))[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "            output = cv2.resize(output, (image.shape[1], L)) #because odd and even numbers and // usage, this line will have to be written better and fixed, you get that\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = np.mean(outputs, 0)\n",
    "        \n",
    "        output[output>0.3] = 1\n",
    "        output[output<0.3] = 0\n",
    "        \n",
    "        preds = []\n",
    "        for _ in output:\n",
    "            classes = np.sum(_, 0)\n",
    "            if np.any(classes):\n",
    "                preds.append(np.argmax(classes)+1)\n",
    "            else:\n",
    "                preds.append(100)\n",
    "                \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a988b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:46.906718Z",
     "start_time": "2023-06-21T16:48:45.265425Z"
    },
    "papermill": {
     "duration": 11.593873,
     "end_time": "2022-10-22T09:12:37.945272",
     "exception": false,
     "start_time": "2022-10-22T09:12:26.351399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['./try2-seg-b1v10-sagview-full/',]# './try7-seresnext50-v7-full/'] #['./try7-b1-v8-full/', './try7-seresnext50-v7-full/']\n",
    "model_funcs = [SagSegModel,]\n",
    "sag_models = []\n",
    "for model_func, folder in zip(model_funcs, folders):\n",
    "    for file in sorted(glob(folder+\"/*\"), key=lambda x: x.split('/')[-1]):\n",
    "        sag_model = SagSegModel()\n",
    "        sag_model.eval()\n",
    "        sag_model.to(device)\n",
    "        st = torch.load(f\"{file}\", map_location=device)['state_dict']\n",
    "        sag_model.load_state_dict(st, strict=False)\n",
    "        sag_models.append(copy.deepcopy(sag_model))\n",
    "\n",
    "sag_models = sag_models#[:1]\n",
    "\n",
    "len(sag_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6dedf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:48.227707Z",
     "start_time": "2023-06-21T16:48:48.221055Z"
    },
    "papermill": {
     "duration": 0.020578,
     "end_time": "2022-10-22T09:12:37.972222",
     "exception": false,
     "start_time": "2022-10-22T09:12:37.951644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BoneSegModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BoneSegModel, self).__init__()\n",
    "        #tf_efficientnet_b0_ns resnest50d_4s2x40d seresnext50_32x4d tf_efficientnetv2_m_in21ft1k\n",
    "        self.feature_extractor = smp.Unet('tu-tf_efficientnet_b1_ns', in_channels=3, classes=8, encoder_weights=None)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        masks = self.feature_extractor(inp)\n",
    "        return masks\n",
    "    \n",
    "def BoneInference(models, images, sz, bs):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        images = nn.functional.interpolate(torch.as_tensor(images).unsqueeze(1), (sz, sz))\n",
    "        \n",
    "        images = torch.cat([images]*3, 1)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        N = images.shape[0]/bs\n",
    "        if not str(N).endswith('.0'): N += 1\n",
    "        N = int(N)\n",
    "        \n",
    "        OUTS = []\n",
    "        for i in range(N):\n",
    "            outs = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    inputs = images[i*bs:(i+1)*bs].to(device)\n",
    "                    \n",
    "                    inputs = inputs.float() / 255\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    outputs = model.sigmoid(outputs)\n",
    "                    outputs = outputs.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                    \n",
    "                    #print(outputs.shape)\n",
    "                    \n",
    "                    outs.append(outputs)\n",
    "                    \n",
    "            outs = np.stack(outs)\n",
    "            \n",
    "            #print(outs.shape)\n",
    "            \n",
    "            outs = np.mean(outs, 0)\n",
    "            \n",
    "            outs[outs>0.5] = 1\n",
    "            outs[outs<=0.5] = 0\n",
    "            \n",
    "            outs = outs.astype(np.uint8)\n",
    "            \n",
    "            #print(outs.shape)\n",
    "            \n",
    "            outs = np.stack(outs)\n",
    "            OUTS.extend(outs)\n",
    "    \n",
    "    OUTS = np.stack(OUTS)\n",
    "    \n",
    "    return OUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06e11f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:50.568843Z",
     "start_time": "2023-06-21T16:48:48.848029Z"
    },
    "papermill": {
     "duration": 8.171515,
     "end_time": "2022-10-22T09:12:46.149727",
     "exception": false,
     "start_time": "2022-10-22T09:12:37.978212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['./try2-seg-b1v1-full/',]# './try7-seresnext50-v7-full/'] #['./try7-b1-v8-full/', './try7-seresnext50-v7-full/']\n",
    "model_funcs = [BoneSegModel,]\n",
    "bone_models = []\n",
    "for model_func, folder in zip(model_funcs, folders):\n",
    "    for file in sorted(glob(folder+\"/*\"), key=lambda x: x.split('/')[-1]):\n",
    "        bone_model = BoneSegModel()\n",
    "        bone_model.eval()\n",
    "        bone_model.to(device)\n",
    "        st = torch.load(f\"{file}\", map_location=device)['state_dict']\n",
    "        bone_model.load_state_dict(st, strict=False)\n",
    "        bone_models.append(copy.deepcopy(bone_model))\n",
    "\n",
    "bone_models = bone_models#[:1]\n",
    "\n",
    "len(bone_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267fc68c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:51.992179Z",
     "start_time": "2023-06-21T16:48:51.976435Z"
    },
    "papermill": {
     "duration": 0.028855,
     "end_time": "2022-10-22T09:12:46.185279",
     "exception": false,
     "start_time": "2022-10-22T09:12:46.156424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.feature_extractor = timm.models.tf_efficientnet_b5_ns(in_chans=3, pretrained=False, num_classes=0, global_pool='')\n",
    "        \n",
    "        f = self.feature_extractor.num_features\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(f, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        features = self.feature_extractor(inp)\n",
    "        features = self.avgpool(features)\n",
    "        features = self.flatten(features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "    \n",
    "class Model2(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.feature_extractor = timm.models.seresnext50_32x4d(in_chans=3, pretrained=False, num_classes=0, global_pool='')\n",
    "        \n",
    "        f = self.feature_extractor.num_features\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(f, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        features = self.feature_extractor(inp)\n",
    "        features = self.avgpool(features)\n",
    "        features = self.flatten(features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class Model3(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.feature_extractor = timm.models.tf_efficientnetv2_s_in21k(in_chans=3, pretrained=False, num_classes=0, global_pool='')\n",
    "        \n",
    "        f = self.feature_extractor.num_features\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(f, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        features = self.feature_extractor(inp)\n",
    "        features = self.avgpool(features)\n",
    "        features = self.flatten(features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "    \n",
    "def CLSInference(models, images, bs):\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        \n",
    "        N = images.shape[0]/bs\n",
    "        if not str(N).endswith('.0'): N += 1\n",
    "        N = int(N)\n",
    "        \n",
    "        OUTS = []\n",
    "        FEATS = []\n",
    "        for i in range(N):\n",
    "            outs = []\n",
    "            feats = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    inputs = images[i*bs:(i+1)*bs].to(device)\n",
    "                    \n",
    "                    inputs = inputs.float() / 255\n",
    "                    \n",
    "                    outputs, features = model(inputs)\n",
    "                    outputs = model.sigmoid(outputs)\n",
    "                    outputs = outputs.detach().cpu().numpy()\n",
    "                    \n",
    "                    features = features.detach().cpu().numpy()\n",
    "                    \n",
    "                    feats.append(features)\n",
    "                    \n",
    "                    outs.append(outputs)\n",
    "                    \n",
    "            outs = np.stack(outs)\n",
    "            feats = np.stack(feats)\n",
    "            \n",
    "            #print(outs.shape)\n",
    "            \n",
    "            outs = np.mean(outs, 0)\n",
    "            feats = np.mean(feats, 0)\n",
    "            \n",
    "            OUTS.extend(outs)\n",
    "            FEATS.extend(feats)\n",
    "    \n",
    "    OUTS = np.stack(OUTS)\n",
    "    FEATS = np.stack(FEATS)\n",
    "    \n",
    "    return OUTS, FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75e192a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:56.640724Z",
     "start_time": "2023-06-21T16:48:53.116857Z"
    },
    "papermill": {
     "duration": 9.660908,
     "end_time": "2022-10-22T09:12:55.852466",
     "exception": false,
     "start_time": "2022-10-22T09:12:46.191558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['./try17-b5-v5-t4-pseudo-round1/',]# './v2s-v6-full-plus-v6-full-tuned']# './try7-seresnext50-v7-full/'] #['./try7-b1-v8-full/',]\n",
    "model_funcs = [Model,]\n",
    "cls_models = []\n",
    "for model_func, folder in tqdm(zip(model_funcs, folders)):\n",
    "    for file in sorted(glob(folder+\"/*\"), key=lambda x: x.split('/')[-1]):\n",
    "        cls_model = model_func()\n",
    "        cls_model.eval()\n",
    "        cls_model.to(device)\n",
    "        st = torch.load(f\"{file}\", map_location=device)#['state_dict']\n",
    "        cls_model.load_state_dict(st, )#strict=False)\n",
    "        cls_models.append(copy.deepcopy(cls_model))\n",
    "\n",
    "cls_models = cls_models#[1:2]\n",
    "\n",
    "len(cls_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706bffad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:57.788235Z",
     "start_time": "2023-06-21T16:48:57.780764Z"
    },
    "papermill": {
     "duration": 0.028903,
     "end_time": "2022-10-22T09:12:55.888224",
     "exception": false,
     "start_time": "2022-10-22T09:12:55.859321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, seq_dim=64):\n",
    "        super(SeqModel, self).__init__()\n",
    "        \n",
    "        base = 2048\n",
    "        m = 1\n",
    "        \n",
    "        self.lstm1 = nn.GRU(base*m, 512*m, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.GRU(512*2*m, 512*m, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.attention1 = Attention(512 * m * 2, seq_dim)\n",
    "        #self.attention2 = UFOAttention(d_model=512*m*2, d_k=512*m*2, d_v=512*m*2, h=8)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(seq_dim, 1, 1)\n",
    "        \n",
    "        self.lstm_bn1 = nn.BatchNorm1d(seq_dim)\n",
    "        self.lstm_bn2 = nn.BatchNorm1d(seq_dim)\n",
    "        \n",
    "        self.att_bn1 = nn.BatchNorm1d(512*2*m)\n",
    "        \n",
    "        self.conv_bn1 = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.clf = nn.Linear(512*2*m*2, 1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.final_classifier = nn.Linear(2048*7, 8)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x, _ = self.lstm1(inp)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.lstm_bn1(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x = self.lstm_bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x_conv = self.conv1(x)\n",
    "        x_conv = x_conv[:, 0]\n",
    "        \n",
    "        #x, _ = self.lstm2(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #max_pool, _ = torch.max(x, 1)\n",
    "        #max_pool = self.avgpool(x.transpose(1, 2))[:, :, 0]\n",
    "        #print(max_pool.shape)\n",
    "        \n",
    "        #x = self.attention2(x, x, x)\n",
    "        att_pool = self.attention1(x, mask=None)\n",
    "        #print(att_pool.shape)\n",
    "        #print(att_pool.shape)\n",
    "        \n",
    "        x = att_pool#torch.cat([max_pool, att_pool], -1)\n",
    "        \n",
    "        x = self.att_bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = torch.cat([x, x_conv], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #logits = self.clf(x)\n",
    "        \n",
    "        features = x.reshape(x.shape[0]//7, 7, 2048)\n",
    "        features = nn.Flatten(1, 2)(features)\n",
    "        \n",
    "        logits = self.final_classifier(features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb46942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:59.967654Z",
     "start_time": "2023-06-21T16:48:59.361535Z"
    },
    "papermill": {
     "duration": 3.521484,
     "end_time": "2022-10-22T09:12:59.416212",
     "exception": false,
     "start_time": "2022-10-22T09:12:55.894728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['./b5-v5-t4-pseudo-round1-seq-v2/',]# './v2s-v6-full-plus-v6-full-tuned']# './try7-seresnext50-v7-full/'] #['./try7-b1-v8-full/',]\n",
    "model_funcs = [SeqModel]\n",
    "dims = [64,]\n",
    "seq_models = []\n",
    "for model_func, folder, d in tqdm(zip(model_funcs, folders, dims)):\n",
    "    for file in sorted(glob(folder+\"/*\"), key=lambda x: x.split('/')[-1]):\n",
    "        seq_model = model_func(d)\n",
    "        seq_model.eval()\n",
    "        seq_model.to(device)\n",
    "        st = torch.load(f\"{file}\", map_location=device)#['state_dict']\n",
    "        seq_model.load_state_dict(st, )#strict=False)\n",
    "        seq_models.append(copy.deepcopy(seq_model))\n",
    "\n",
    "seq_models = seq_models#[1:2]\n",
    "\n",
    "len(seq_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0970d158-eb47-4d98-b298-8692da0eb377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(INPUT_TEST_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab4aa3a-e3d5-4609-898e-f126021a747b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06892402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T17:14:18.983150Z",
     "start_time": "2023-06-21T17:07:48.260798Z"
    },
    "papermill": {
     "duration": 96.796658,
     "end_time": "2022-10-22T09:14:36.259734",
     "exception": false,
     "start_time": "2022-10-22T09:12:59.463076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 92/5161 [13:31<12:05:08,  8.58s/it]"
     ]
    }
   ],
   "source": [
    "row_ids = []\n",
    "fractured = []\n",
    "\n",
    "augs = A.Compose([\n",
    "    #A.Resize(CFG.SZ_H, CFG.SZ_W),\n",
    "    A.LongestMaxSize(1024),\n",
    "    A.PadIfNeeded(1024, 1024, border_mode=0, p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "means = [0.4760, 0.0723, 0.1412, 0.0362, 0.0535, 0.0802, 0.1372, 0.1947]\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    study_id = row['StudyInstanceUID']\n",
    "    path = row['image_folder']\n",
    "    files = glob(path + \"/*\")\n",
    "    files.sort(key=lambda x: int(re.findall(r'(\\d+).d', x)[0]))\n",
    "\n",
    "    try:\n",
    "    #if 1:\n",
    "        dl = DataLoader(LoadDicoms(files), batch_size=128, num_workers=4, shuffle=False)\n",
    "\n",
    "        #FAST LOAD ALL IMAGES\n",
    "\n",
    "        images = []\n",
    "        z_pos = []\n",
    "        for batch in dl:\n",
    "            images.extend(batch[0])\n",
    "            z_pos.extend(batch[1])\n",
    "        images = torch.stack(images).numpy()\n",
    "        z_pos = torch.stack(z_pos).numpy()\n",
    "\n",
    "        z_inter = np.sort(z_pos)[3] - np.sort(z_pos)[2]\n",
    "\n",
    "        images = images[np.argsort(-z_pos)]\n",
    "\n",
    "        #SAG VIEW, SELECTS IMPORTANT ONLY IMAGES AND THEIR KEYS\n",
    "\n",
    "        sag = images[:, :, images.shape[-1]//2]\n",
    "\n",
    "        keys = np.array(SagInference(sag_models, sag, images.shape[0]))\n",
    "        selec_idxs = np.where(np.logical_and(keys!=100, keys!=8))\n",
    "\n",
    "        images = images[selec_idxs]\n",
    "        keys = keys[selec_idxs]\n",
    "\n",
    "        #GET MASKS, AND ONLY THOSE WHICH HAVE MASKS\n",
    "\n",
    "        masks = BoneInference(bone_models, images, 256, 32)\n",
    "\n",
    "        masks = np.max(masks, -1)\n",
    "\n",
    "        selec_idxs = [i for i, m in enumerate(masks) if np.max(m)]\n",
    "\n",
    "        masks = masks[selec_idxs]\n",
    "        images = images[selec_idxs]\n",
    "        keys = keys[selec_idxs]\n",
    "\n",
    "        #PREPROCESSING FOR IMAGES, INCLUDES ROI, AND 1:-1 TO MAKE SURE ALL 2.5D IS CORRECT, DID SAME WITH KEYS FOR MAKING SURE ITS GOOD FOR THE UPCOMING STEP\n",
    "\n",
    "        inputs = []\n",
    "        for i in range(1, len(images)-1):\n",
    "            image_curr = images[i]\n",
    "            image_last = images[i-1]\n",
    "            image_next = images[i+1]\n",
    "\n",
    "            image = np.stack([image_last, image_curr, image_next], -1)\n",
    "\n",
    "            mask = masks[i]\n",
    "\n",
    "            try:\n",
    "                ymin, ymax = np.min(np.where(mask)[0])/mask.shape[1], np.max(np.where(mask)[0])/mask.shape[1]\n",
    "                xmin, xmax = np.min(np.where(mask)[1])/mask.shape[0], np.max(np.where(mask)[1])/mask.shape[0]\n",
    "\n",
    "                xmin = xmin * 0.95\n",
    "                ymin = ymin * 0.95\n",
    "                xmax = xmax * 1.05\n",
    "                ymax = ymax * 1.05\n",
    "\n",
    "                image = image[int(ymin*image.shape[0]):int(ymax*image.shape[0]), int(xmin*image.shape[1]):int(xmax*image.shape[1])]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            #print(image.shape)\n",
    "\n",
    "            image = augs(image=image)['image']\n",
    "\n",
    "            inputs.append(image)\n",
    "\n",
    "        images = torch.stack(inputs)\n",
    "\n",
    "        keys = keys[1:-1]\n",
    "\n",
    "        #FEED RESIZE IMAGES TO MODELS TO GET FEATURES AND PREDS\n",
    "        inputs = nn.functional.interpolate(images, (456, 456))\n",
    "        preds, features = CLSInference(cls_models[:5], inputs, 32)\n",
    "\n",
    "        #MAP PREDS TO BONES AND DO POST\n",
    "        bone_features = []\n",
    "        dim = 64\n",
    "        \n",
    "        for bone in range(1, 8):\n",
    "            features_ = np.zeros((dim, 2048))\n",
    "            \n",
    "            if np.sum(keys==bone):\n",
    "                feats = features[keys==bone]\n",
    "                features_[:min(len(feats), dim)] = feats[:min(len(feats), dim)]\n",
    "\n",
    "            bone_features.append(features_)\n",
    "\n",
    "\n",
    "        \n",
    "        bone_features = torch.as_tensor(np.stack(bone_features)).float().to(device)\n",
    "        sps = []\n",
    "        for seq_model in seq_models[:5]:\n",
    "            sp = seq_model.sigmoid(seq_model(bone_features[:, :])[0]).detach().cpu().numpy()\n",
    "            sps.append(sp)\n",
    "        sp = np.mean(sps, 0)\n",
    "        \n",
    "        for _ in range(1, 8):\n",
    "            row_ids.append(f\"{study_id}_C{_}\")\n",
    "            fractured.append(sp[_])\n",
    "        \n",
    "        row_ids.append(f\"{study_id}_patient_overall\")\n",
    "        fractured.append(sp[0])\n",
    "\n",
    "        import gc\n",
    "        gc.enable()\n",
    "\n",
    "        del preds, features, images, inputs, masks\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "    #'''\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(traceback.format_exc())\n",
    "        L = len(fractured)\n",
    "        if L%8:\n",
    "            row_ids = row_ids[:L - (L%8)]\n",
    "            #fractured = fractured[:L - (L%8)]\n",
    "        \n",
    "        for bone in range(1, 8):\n",
    "            row_ids.append(f\"{study_id}_C{bone}\")\n",
    "        \n",
    "        row_ids.append(f\"{study_id}_patient_overall\")\n",
    "        \n",
    "        fractured.extend(means[1:][L%8:])\n",
    "        fractured.append(means[0])\n",
    "    #'''\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddd2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T17:14:18.990764Z",
     "start_time": "2023-06-21T17:14:18.988115Z"
    },
    "papermill": {
     "duration": 0.038435,
     "end_time": "2022-10-22T09:14:36.349472",
     "exception": false,
     "start_time": "2022-10-22T09:14:36.311037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'row_id': row_ids, 'fractured': fractured})\n",
    "sub.fractured = sub.fractured.clip(0.001, 0.999)\n",
    "#sub.fractured = sub.fractured.clip(0.1, 0.9)\n",
    "sub.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6f57c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T17:14:18.994549Z",
     "start_time": "2023-06-21T17:14:18.991556Z"
    },
    "papermill": {
     "duration": 0.019449,
     "end_time": "2022-10-22T09:14:36.377571",
     "exception": false,
     "start_time": "2022-10-22T09:14:36.358122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2007ca7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp39env",
   "language": "python",
   "name": "cp39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 239.303199,
   "end_time": "2022-10-22T09:14:39.191127",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-22T09:10:39.887928",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
