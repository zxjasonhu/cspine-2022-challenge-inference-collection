{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374906d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### please specify your input path here\n",
    "INPUT_TEST_FILE = \"YOUR_TEST_FILE\" # csv file list locations / paths to test cases (dicom)\n",
    "OUTPUT_FILE = \"YOUR_OUTPUT\" # in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b0debf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:04:55.526480Z",
     "start_time": "2023-06-22T17:04:55.518752Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 62.735602,
     "end_time": "2022-10-27T04:00:39.183136",
     "exception": false,
     "start_time": "2022-10-27T03:59:36.447534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path = [\n",
    "    './covn3d-same',\n",
    "    './timm20221011/pytorch-image-models-master',\n",
    "    './smp20210127/segmentation_models.pytorch-master/segmentation_models.pytorch-master',\n",
    "    './smp20210127/pretrained-models.pytorch-master/pretrained-models.pytorch-master',\n",
    "    './smp20210127/EfficientNet-PyTorch-master/EfficientNet-PyTorch-master',\n",
    "] + sys.path\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEBUG = False\n",
    "# !pip -q install ./pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "# !pip -q install ./pylibjpeg140py3/python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e503f6ba-fd66-433f-b4aa-55c55ee9e0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570c922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:14.939105Z",
     "start_time": "2023-06-22T17:05:04.602159Z"
    },
    "papermill": {
     "duration": 8.668583,
     "end_time": "2022-10-27T04:00:47.858242",
     "exception": false,
     "start_time": "2022-10-27T04:00:39.189659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import timm4smp\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "timm.__version__, timm4smp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1dc0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:22.051545Z",
     "start_time": "2023-06-22T17:05:22.045282Z"
    },
    "papermill": {
     "duration": 0.015741,
     "end_time": "2022-10-27T04:00:47.880668",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.864927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size_seg = (128, 128, 128)\n",
    "msk_size = image_size_seg[0]\n",
    "image_size_cls = 512  # 这里需要固定 512\n",
    "n_slice_per_c = 15\n",
    "n_ch = 5\n",
    "\n",
    "batch_size_seg = 1\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bab12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:39.656052Z",
     "start_time": "2023-06-22T17:05:39.633648Z"
    },
    "papermill": {
     "duration": 0.03906,
     "end_time": "2022-10-27T04:00:47.925892",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.886832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(INPUT_TEST_FILE).iloc[:1]\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_TEST_FILE)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010c7af",
   "metadata": {
    "papermill": {
     "duration": 0.005923,
     "end_time": "2022-10-27T04:00:47.938114",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.932191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0eb8b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:53.331198Z",
     "start_time": "2023-06-22T17:05:53.323004Z"
    },
    "papermill": {
     "duration": 0.019998,
     "end_time": "2022-10-27T04:00:47.964218",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.944220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.segDataset import SegTestDataset\n",
    "dataset_seg = SegTestDataset(df)\n",
    "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3fc6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:57.960072Z",
     "start_time": "2023-06-22T17:05:57.949620Z"
    },
    "papermill": {
     "duration": 0.015739,
     "end_time": "2022-10-27T04:00:48.015337",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.999598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, :, :, 60]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27f2965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:58.174085Z",
     "start_time": "2023-06-22T17:05:58.167416Z"
    },
    "papermill": {
     "duration": 0.01506,
     "end_time": "2022-10-27T04:00:48.036612",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.021552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, :, 60, :]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b666a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:58.416646Z",
     "start_time": "2023-06-22T17:05:58.413557Z"
    },
    "papermill": {
     "duration": 0.015233,
     "end_time": "2022-10-27T04:00:48.059006",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.043773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, 60, :, :]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee54dc3",
   "metadata": {
    "papermill": {
     "duration": 0.005922,
     "end_time": "2022-10-27T04:00:48.071907",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.065985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92296e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:06:00.574090Z",
     "start_time": "2023-06-22T17:06:00.557436Z"
    },
    "papermill": {
     "duration": 0.055608,
     "end_time": "2022-10-27T04:00:48.133626",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.078018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm4smp.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "#             padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "#         print(module.padding, module_output.padding)\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm4smp.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "#         for f in global_features[1:]:\n",
    "#             print(f.shape)\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, backbone, image_size, pretrained=False):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=0,\n",
    "            drop_path_rate=0,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone or 'nfnet' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * 7, in_chans, self.image_size, self.image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * 7, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Timm1BoneModel(nn.Module):\n",
    "    def __init__(self, backbone, image_size, pretrained=False):\n",
    "        super(Timm1BoneModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=0,\n",
    "            drop_path_rate=0,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone or 'nfnet' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c, in_chans, self.image_size, self.image_size)\n",
    "        # feat = self.preencoder(x)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c, -1)\n",
    "        feat, _ = self.lstm(feat)\n",
    "        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n",
    "        feat = self.head(feat)\n",
    "        feat = feat.view(bs, n_slice_per_c).contiguous()\n",
    "\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d46c8",
   "metadata": {
    "papermill": {
     "duration": 0.005908,
     "end_time": "2022-10-27T04:00:48.145949",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.140041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddabf1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:07:19.131821Z",
     "start_time": "2023-06-22T17:07:13.105412Z"
    },
    "papermill": {
     "duration": 23.318826,
     "end_time": "2022-10-27T04:01:11.470883",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.152057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_seg = []\n",
    "\n",
    "kernel_type = 'timm3d_v2s_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_mixup1_lr1e3_20x50ep'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "model_dir_seg = './seg-v2s-0911/'\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "\n",
    "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "backbone = 'resnet18d'\n",
    "model_dir_seg = './segres18d0920/'\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "len(models_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc61b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:07:26.347216Z",
     "start_time": "2023-06-22T17:07:20.988075Z"
    },
    "papermill": {
     "duration": 15.105215,
     "end_time": "2022-10-27T04:01:26.582780",
     "exception": false,
     "start_time": "2022-10-27T04:01:11.477565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_2d_lstmv22headv2_convnn_512_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr6e6_eta6e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './cls-convnn512-1011'\n",
    "backbone = 'convnext_nano'\n",
    "in_chans = 6\n",
    "models_cls = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = TimmModel(backbone, image_size=512, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "    \n",
    "\n",
    "kernel_type = '0920_2d_lstmv22headv2_convnpc_512_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr6e6_eta6e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsconvnpc5121023'\n",
    "backbone = 'convnext_pico_ols'\n",
    "for fold in [3, 4]:\n",
    "    model = TimmModel(backbone, image_size=512, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "\n",
    "\n",
    "kernel_type = '0920_2d_lstmv22headv2_convnt_384_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr10e6_eta10e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsconvnt1019'\n",
    "backbone = 'convnext_tiny_in22ft1k'\n",
    "for fold in [5, 6]:\n",
    "    model = TimmModel(backbone, image_size=384, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "    \n",
    "    \n",
    "kernel_type = '0920_2d_lstmv22headv2_nfl0_384_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr15e6_eta15e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsnfl03841026'\n",
    "backbone = 'eca_nfnet_l0'\n",
    "for fold in [1, 9]:\n",
    "    model = TimmModel(backbone, image_size=384, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "\n",
    "\n",
    "len(models_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e8954f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:13:26.648142Z",
     "start_time": "2023-06-22T17:13:21.080148Z"
    },
    "papermill": {
     "duration": 15.940121,
     "end_time": "2022-10-27T04:01:42.529429",
     "exception": false,
     "start_time": "2022-10-27T04:01:26.589308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_1bonev2_effv2s_512_15_6ch_augv2_mixupp6_dpr2_drl3_rov1p2_bs7_lr20e6_eta20e6_75ep'\n",
    "model_dir_cls = './cls-1bone-v2b3-512-1017/'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "in_chans = 6\n",
    "models_bone = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = Timm1BoneModel(backbone, image_size=512, pretrained=False)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location='cpu')\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    models_bone.append(model)\n",
    "\n",
    "\n",
    "kernel_type = '0920_1bonev2_convnt_384_15_6ch_augv2_mixupp5_dpr3_drl3_rov1p2_bs21_lr14e6_eta14e6_50ep_dp'\n",
    "model_dir_cls = './cls-1bone-convnt-384-1021'\n",
    "backbone = 'convnext_tiny_in22ft1k'\n",
    "for fold in [0,2,5,8,9]:\n",
    "    model = Timm1BoneModel(backbone, image_size=384, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location='cpu')\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    models_bone.append(model)\n",
    "\n",
    "\n",
    "len(models_bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f9c1e9-d33d-4b57-b8b8-babf97f82ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:13:29.240279Z",
     "start_time": "2023-06-22T17:13:29.235852Z"
    },
    "papermill": {
     "duration": 0.035583,
     "end_time": "2022-10-27T04:01:43.633055",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.597472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_bone(msk, cid, t_paths, cropped_images):\n",
    "    n_scans = len(t_paths)\n",
    "    bone = []\n",
    "    try:\n",
    "        msk_b = msk[cid] > 0.2\n",
    "        msk_c = msk[cid] > 0.05\n",
    "\n",
    "        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n",
    "        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n",
    "            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
    "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
    "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n",
    "\n",
    "\n",
    "        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n",
    "        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n",
    "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n",
    "\n",
    "            msk_this = msk[cid, :, :, ind_]\n",
    "\n",
    "            images = []\n",
    "            for i in range(-n_ch//2+1, n_ch//2+1):\n",
    "                try:\n",
    "                    dicom = pydicom.read_file(t_paths[ind+i])\n",
    "                    images.append(dicom.pixel_array)\n",
    "                except:\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)\n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "            msk_this = (msk_this * 255).astype(np.uint8)\n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n",
    "\n",
    "            bone.append(torch.tensor(data))\n",
    "\n",
    "    except:\n",
    "        for sid in range(n_slice_per_c):\n",
    "            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n",
    "\n",
    "    cropped_images[cid] = torch.stack(bone, 0)\n",
    "\n",
    "\n",
    "def load_cropped_images(msk, image_folder, n_ch=n_ch):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0].split(\"-\")[-1]))\n",
    "    for cid in range(7):\n",
    "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images))\n",
    "        threads[cid].start()\n",
    "    for cid in range(7):\n",
    "        threads[cid].join()\n",
    "\n",
    "    return torch.cat(cropped_images, 0)\n",
    "\n",
    "\n",
    "def get_trans(img, I):\n",
    "    I = I % 8\n",
    "    if I >= 4:\n",
    "        img = img.transpose(3, 4)\n",
    "    if I % 4 == 0:\n",
    "        return img\n",
    "    elif I % 4 == 1:\n",
    "        return img.flip(3)\n",
    "    elif I % 4 == 2:\n",
    "        return img.flip(4)\n",
    "    elif I % 4 == 3:\n",
    "        return img.flip(3).flip(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898e6f2",
   "metadata": {
    "papermill": {
     "duration": 0.006363,
     "end_time": "2022-10-27T04:01:43.646011",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.639648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9f58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:21:54.591060Z",
     "start_time": "2023-06-22T17:13:31.927866Z"
    },
    "papermill": {
     "duration": 90.776009,
     "end_time": "2022-10-27T04:03:14.428601",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.652592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 410/5161 [1:06:34<28:21:35, 21.49s/it]"
     ]
    }
   ],
   "source": [
    "outputs1 = []\n",
    "outputs2 = []\n",
    "\n",
    "bar = tqdm(loader_seg)\n",
    "with torch.no_grad():\n",
    "#     with amp.autocast():\n",
    "        for batch_id, (images) in enumerate(bar):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # SEG\n",
    "            pred_masks = []\n",
    "            for model in models_seg:\n",
    "                pmask = model(images).sigmoid()\n",
    "                pred_masks.append(pmask)\n",
    "#                 print(pmask)\n",
    "            pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n",
    "\n",
    "            # Build cls input\n",
    "            cls_inp = []\n",
    "            threads = [None] * 7\n",
    "            cropped_images = [None] * 7\n",
    "\n",
    "            for i in range(pred_masks.shape[0]):\n",
    "                row = df.iloc[batch_id*batch_size_seg+i]\n",
    "                cropped_images = load_cropped_images(pred_masks[i], row.image_folder)\n",
    "                cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n",
    "            cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 512, 512)\n",
    "\n",
    "            pred_cls1, pred_cls2 = [], []\n",
    "            # CLS 512\n",
    "            for I, model in enumerate(models_cls[:7]):\n",
    "                logits, logits2 = model(get_trans(cls_inp, I))\n",
    "                pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "                pred_cls2.append(logits2.sigmoid())\n",
    "\n",
    "            # CLS 1 bone 512\n",
    "            cls_inp = cls_inp.view(7, 15, 6, 512, 512).contiguous()\n",
    "            for I, model in enumerate(models_bone[:5]):\n",
    "                logits = model(get_trans(cls_inp, I))\n",
    "                pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "\n",
    "            ### 512 -> 384\n",
    "            cls_inp = torch.stack([F.interpolate(cls_inp[i], size=384, mode='bilinear') for i in range(7)], 0)\n",
    "            ###\n",
    "            \n",
    "            # CLS 1 bone 384\n",
    "            for I, model in enumerate(models_bone[5:]):\n",
    "                logits = model(get_trans(cls_inp, I+5))\n",
    "                pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "\n",
    "            # CLS 384\n",
    "            cls_inp = cls_inp.view(-1, 105, 6, 384, 384).contiguous()\n",
    "            for I, model in enumerate(models_cls[7:]):\n",
    "                logits, logits2 = model(get_trans(cls_inp, I+7))\n",
    "                pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "                pred_cls2.append(logits2.sigmoid())\n",
    "\n",
    "\n",
    "            pred_cls1 = torch.stack(pred_cls1, 0).mean(0)\n",
    "            pred_cls2 = torch.stack(pred_cls2, 0).mean(0)\n",
    "            outputs1.append(pred_cls1.cpu())\n",
    "            outputs2.append(pred_cls2.cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07aa4c",
   "metadata": {
    "papermill": {
     "duration": 0.006783,
     "end_time": "2022-10-27T04:03:14.442464",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.435681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826e26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:22:01.652621Z",
     "start_time": "2023-06-22T17:22:01.645617Z"
    },
    "papermill": {
     "duration": 0.015657,
     "end_time": "2022-10-27T04:03:14.464950",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.449293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs1 = torch.cat(outputs1)\n",
    "outputs2 = torch.cat(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba8589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:22:03.553959Z",
     "start_time": "2023-06-22T17:22:03.538133Z"
    },
    "papermill": {
     "duration": 0.022762,
     "end_time": "2022-10-27T04:03:14.494472",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.471710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRED1 = (outputs1.sort(-1).values[:, :, -5:].mean(-1)).clamp(0.0001, 0.9999)\n",
    "\n",
    "p_prob    = 1 - torch.prod(1 - (outputs1.sort(-1).values[:, :, -1]), 1)\n",
    "p_prob_v2 = outputs1.sort(-1).values[:, :, -1].max(1).values\n",
    "PRED2 = (p_prob * 0.4 + p_prob_v2 * 0.01 + outputs2.view(-1) * 0.6).clamp(0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe606d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:22:06.833907Z",
     "start_time": "2023-06-22T17:22:06.825366Z"
    },
    "papermill": {
     "duration": 0.016183,
     "end_time": "2022-10-27T04:03:14.518168",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.501985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_ids = []\n",
    "for _, row in df.iterrows():\n",
    "    for i in range(7):\n",
    "        row_ids.append(row.StudyInstanceUID + f'_C{i+1}')\n",
    "    row_ids.append(row.StudyInstanceUID + '_patient_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55364f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:22:07.363479Z",
     "start_time": "2023-06-22T17:22:07.357518Z"
    },
    "papermill": {
     "duration": 0.016162,
     "end_time": "2022-10-27T04:03:14.541341",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.525179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    'row_id': row_ids,\n",
    "    'fractured': torch.cat([PRED1, PRED2.unsqueeze(1)], 1).view(-1),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069dcf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:22:08.551361Z",
     "start_time": "2023-06-22T17:22:08.539150Z"
    },
    "papermill": {
     "duration": 0.021793,
     "end_time": "2022-10-27T04:03:14.595636",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.573843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65902f1",
   "metadata": {
    "papermill": {
     "duration": 0.006999,
     "end_time": "2022-10-27T04:03:14.609934",
     "exception": false,
     "start_time": "2022-10-27T04:03:14.602935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba39ef-7145-42b1-9680-7014dcf215b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp39env",
   "language": "python",
   "name": "cp39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 228.336534,
   "end_time": "2022-10-27T04:03:17.394700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-27T03:59:29.058166",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
