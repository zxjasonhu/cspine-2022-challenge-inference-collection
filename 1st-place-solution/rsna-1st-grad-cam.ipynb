{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### please specify your input path here\n",
    "INPUT_TEST_FILE = \"YOUR_TEST_FILE\" # csv file list locations / paths to test cases (dicom)\n",
    "OUTPUT_FOLDER = \"output_folder\" # folder to store the output cam files in npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b0debf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:04:55.526480Z",
     "start_time": "2023-06-22T17:04:55.518752Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 62.735602,
     "end_time": "2022-10-27T04:00:39.183136",
     "exception": false,
     "start_time": "2022-10-27T03:59:36.447534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path = [\n",
    "    './covn3d-same',\n",
    "    './timm20221011/pytorch-image-models-master',\n",
    "    './smp20210127/segmentation_models.pytorch-master/segmentation_models.pytorch-master',\n",
    "    './smp20210127/pretrained-models.pytorch-master/pretrained-models.pytorch-master',\n",
    "    './smp20210127/EfficientNet-PyTorch-master/EfficientNet-PyTorch-master',\n",
    "] + sys.path\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEBUG = False\n",
    "# !pip -q install ./pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "# !pip -q install ./pylibjpeg140py3/python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e503f6ba-fd66-433f-b4aa-55c55ee9e0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570c922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:14.939105Z",
     "start_time": "2023-06-22T17:05:04.602159Z"
    },
    "papermill": {
     "duration": 8.668583,
     "end_time": "2022-10-27T04:00:47.858242",
     "exception": false,
     "start_time": "2022-10-27T04:00:39.189659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import timm4smp\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "timm.__version__, timm4smp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1dc0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:22.051545Z",
     "start_time": "2023-06-22T17:05:22.045282Z"
    },
    "papermill": {
     "duration": 0.015741,
     "end_time": "2022-10-27T04:00:47.880668",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.864927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size_seg = (128, 128, 128)\n",
    "msk_size = image_size_seg[0]\n",
    "image_size_cls = 512  # 这里需要固定 512\n",
    "n_slice_per_c = 15\n",
    "n_ch = 5\n",
    "\n",
    "batch_size_seg = 1\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bab12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:39.656052Z",
     "start_time": "2023-06-22T17:05:39.633648Z"
    },
    "papermill": {
     "duration": 0.03906,
     "end_time": "2022-10-27T04:00:47.925892",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.886832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(INPUT_TEST_FILE)\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_TEST_FILE).iloc[:1]\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010c7af",
   "metadata": {
    "papermill": {
     "duration": 0.005923,
     "end_time": "2022-10-27T04:00:47.938114",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.932191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0eb8b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:53.331198Z",
     "start_time": "2023-06-22T17:05:53.323004Z"
    },
    "papermill": {
     "duration": 0.019998,
     "end_time": "2022-10-27T04:00:47.964218",
     "exception": false,
     "start_time": "2022-10-27T04:00:47.944220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.segDataset import SegTestDataset\n",
    "dataset_seg = SegTestDataset(df)\n",
    "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee54dc3",
   "metadata": {
    "papermill": {
     "duration": 0.005922,
     "end_time": "2022-10-27T04:00:48.071907",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.065985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92296e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:06:00.574090Z",
     "start_time": "2023-06-22T17:06:00.557436Z"
    },
    "papermill": {
     "duration": 0.055608,
     "end_time": "2022-10-27T04:00:48.133626",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.078018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm4smp.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "#             padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "#         print(module.padding, module_output.padding)\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm4smp.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "#         for f in global_features[1:]:\n",
    "#             print(f.shape)\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, backbone, image_size, pretrained=False):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=0,\n",
    "            drop_path_rate=0,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone or 'nfnet' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * 7, in_chans, self.image_size, self.image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * 7, -1)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "        out = self.head2(feat2[:, 0])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d46c8",
   "metadata": {
    "papermill": {
     "duration": 0.005908,
     "end_time": "2022-10-27T04:00:48.145949",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.140041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddabf1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:07:19.131821Z",
     "start_time": "2023-06-22T17:07:13.105412Z"
    },
    "papermill": {
     "duration": 23.318826,
     "end_time": "2022-10-27T04:01:11.470883",
     "exception": false,
     "start_time": "2022-10-27T04:00:48.152057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_seg = []\n",
    "\n",
    "kernel_type = 'timm3d_v2s_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_mixup1_lr1e3_20x50ep'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "model_dir_seg = './seg-v2s-0911/'\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "\n",
    "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "backbone = 'resnet18d'\n",
    "model_dir_seg = './segres18d0920/'\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "len(models_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc61b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:07:26.347216Z",
     "start_time": "2023-06-22T17:07:20.988075Z"
    },
    "papermill": {
     "duration": 15.105215,
     "end_time": "2022-10-27T04:01:26.582780",
     "exception": false,
     "start_time": "2022-10-27T04:01:11.477565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_2d_lstmv22headv2_convnn_512_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr6e6_eta6e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './cls-convnn512-1011'\n",
    "backbone = 'convnext_nano'\n",
    "in_chans = 6\n",
    "models_cls = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = TimmModel(backbone, image_size=512, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "    \n",
    "\n",
    "kernel_type = '0920_2d_lstmv22headv2_convnpc_512_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr6e6_eta6e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsconvnpc5121023'\n",
    "backbone = 'convnext_pico_ols'\n",
    "for fold in [3, 4]:\n",
    "    model = TimmModel(backbone, image_size=512, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "\n",
    "\n",
    "kernel_type = '0920_2d_lstmv22headv2_convnt_384_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr10e6_eta10e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsconvnt1019'\n",
    "backbone = 'convnext_tiny_in22ft1k'\n",
    "for fold in [5, 6]:\n",
    "    model = TimmModel(backbone, image_size=384, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "    \n",
    "    \n",
    "kernel_type = '0920_2d_lstmv22headv2_nfl0_384_15_6ch_lossv3_augv2_mixupv3p5_dpr3_drl3_rov1p2_rov3p2_bs4_lr15e6_eta15e6_lw151_50ep_ddp'\n",
    "model_dir_cls = './clsnfl03841026'\n",
    "backbone = 'eca_nfnet_l0'\n",
    "for fold in [1, 9]:\n",
    "    model = TimmModel(backbone, image_size=384, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location=device)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls.append(model)\n",
    "\n",
    "\n",
    "len(models_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f9c1e9-d33d-4b57-b8b8-babf97f82ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:13:29.240279Z",
     "start_time": "2023-06-22T17:13:29.235852Z"
    },
    "papermill": {
     "duration": 0.035583,
     "end_time": "2022-10-27T04:01:43.633055",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.597472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_bone(msk, cid, t_paths, cropped_images, crop_info):\n",
    "    n_scans = len(t_paths)\n",
    "    bone = []\n",
    "    try:\n",
    "        msk_b = msk[cid] > 0.2\n",
    "        msk_c = msk[cid] > 0.05\n",
    "\n",
    "        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n",
    "        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n",
    "            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
    "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
    "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n",
    "\n",
    "\n",
    "        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n",
    "        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n",
    "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n",
    "\n",
    "            msk_this = msk[cid, :, :, ind_]\n",
    "\n",
    "            images = []\n",
    "            for i in range(-n_ch//2+1, n_ch//2+1):\n",
    "                try:\n",
    "                    dicom = pydicom.read_file(t_paths[ind+i])\n",
    "                    images.append(dicom.pixel_array)\n",
    "                except:\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)\n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "            msk_this = (msk_this * 255).astype(np.uint8)\n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n",
    "\n",
    "            bone.append(torch.tensor(data))      \n",
    "        crop_info[cid] = [xx1, xx2, yy1, yy2, zz1, zz2, n_scans]\n",
    "             \n",
    "    except:\n",
    "        for sid in range(n_slice_per_c):\n",
    "            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n",
    "\n",
    "    cropped_images[cid] = torch.stack(bone, 0)\n",
    "    \n",
    "\n",
    "\n",
    "def load_cropped_images(msk, image_folder, n_ch=n_ch):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0].split(\"-\")[-1]))\n",
    "    for cid in range(7):\n",
    "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images, crop_info))\n",
    "        threads[cid].start()\n",
    "    for cid in range(7):\n",
    "        threads[cid].join()\n",
    "\n",
    "    return torch.cat(cropped_images, 0), crop_info\n",
    "\n",
    "\n",
    "def get_trans(img, I):\n",
    "    I = I % 8\n",
    "    if I >= 4:\n",
    "        img = img.transpose(3, 4)\n",
    "    if I % 4 == 0:\n",
    "        return img\n",
    "    elif I % 4 == 1:\n",
    "        return img.flip(3)\n",
    "    elif I % 4 == 2:\n",
    "        return img.flip(4)\n",
    "    elif I % 4 == 3:\n",
    "        return img.flip(3).flip(4)\n",
    "    \n",
    "    \n",
    "def get_cam_trans(cam, I):\n",
    "    I = I % 8\n",
    "    if I >= 4:\n",
    "        cam = cam.transpose(1, 2)\n",
    "    if I % 4 == 0:\n",
    "        return cam\n",
    "    elif I % 4 == 1:\n",
    "        return cam.flip(1)\n",
    "    elif I % 4 == 2:\n",
    "        return cam.flip(2)\n",
    "    elif I % 4 == 3:\n",
    "        return cam.flip(1).flip(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898e6f2",
   "metadata": {
    "papermill": {
     "duration": 0.006363,
     "end_time": "2022-10-27T04:01:43.646011",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.639648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa3be25-77b8-4d08-9c2f-7ca53126db78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
    "# 0:9 encoder.stages[-1].blocks[-1]\n",
    "# 9:11 encoder.final_act\n",
    "\n",
    "sys.path.append('../custom_grad_cam/') # customized grad cam lib\n",
    "from interpolate_voxels import cam_to_intermediate_cam\n",
    "\n",
    "targets = [BinaryClassifierOutputTarget(1)] # BinaryClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0fe7576-9070-4d51-991f-0b3974862a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cam(model, target_layers, input_tensor, I):\n",
    "    with GradCAM(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n",
    "        # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "        local_cam = get_cam_trans(torch.tensor(cam(input_tensor=get_trans(input_tensor, I), targets=targets)), I)\n",
    "        if local_cam.shape[-1] != 512:\n",
    "            local_cam = F.interpolate(local_cam.unsqueeze(0), size=512, mode='bilinear').squeeze(0)\n",
    "        local_cam = local_cam.cpu().numpy()\n",
    "        \n",
    "    return local_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c9f58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:21:54.591060Z",
     "start_time": "2023-06-22T17:13:31.927866Z"
    },
    "papermill": {
     "duration": 90.776009,
     "end_time": "2022-10-27T04:03:14.428601",
     "exception": false,
     "start_time": "2022-10-27T04:01:43.652592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [33:32<00:00, 19.73s/it]\n"
     ]
    }
   ],
   "source": [
    "bar = tqdm(loader_seg)\n",
    "# with torch.no_grad():\n",
    "#     with amp.autocast():\n",
    "for batch_id, (images) in enumerate(bar):\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    images = images.to(device)\n",
    "\n",
    "    # SEG\n",
    "    pred_masks = []\n",
    "    for model in models_seg:\n",
    "        pmask = model(images).sigmoid()\n",
    "        pred_masks.append(pmask)\n",
    "#                 print(pmask)\n",
    "    pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().detach().numpy()\n",
    "\n",
    "    # Build cls input\n",
    "    cls_inp = []\n",
    "    threads = [None] * 7\n",
    "    cropped_images = [None] * 7\n",
    "    crop_info = [None] * 7\n",
    "    \n",
    "    name = None\n",
    "    for i in range(pred_masks.shape[0]):\n",
    "        row = df.iloc[batch_id*batch_size_seg+i]\n",
    "        cropped_images, crop_info = load_cropped_images(pred_masks[i], row.image_folder)\n",
    "        cube_id = row['StudyInstanceUID']\n",
    "        cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n",
    "    cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 512, 512)\n",
    "\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    grayscale_cam = np.zeros((105, 512, 512))\n",
    "    # CLS 512\n",
    "    for I, model in enumerate(models_cls[:7]):\n",
    "        grayscale_cam += get_cam(model, [model.encoder.stages[-1].blocks[-1]], cls_inp, I)\n",
    "\n",
    "    ### 512 -> 384\n",
    "    cls_inp = cls_inp.view(7, 15, 6, 512, 512).contiguous()\n",
    "    cls_inp = torch.stack([F.interpolate(cls_inp[i], size=384, mode='bilinear') for i in range(7)], 0)\n",
    "    ###\n",
    "\n",
    "    # CLS 384\n",
    "    cls_inp = cls_inp.view(-1, 105, 6, 384, 384).contiguous()\n",
    "    for I, model in enumerate(models_cls[7:9]):\n",
    "        grayscale_cam += get_cam(model, [model.encoder.stages[-1].blocks[-1]], cls_inp, I+7)\n",
    "\n",
    "    for I, model in enumerate(models_cls[9:]):\n",
    "        grayscale_cam += get_cam(model, [model.encoder.final_act], cls_inp, I+9)\n",
    "\n",
    "    grayscale_cam = grayscale_cam / 11\n",
    "\n",
    "\n",
    "    n_scans = -1\n",
    "    for c_info in crop_info:\n",
    "        if c_info is not None:\n",
    "            n_scans = c_info[-1]\n",
    "            break\n",
    "    if n_scans == -1:\n",
    "        print(f\"Cannot Process case {cube_id}, skip for now\")\n",
    "    \n",
    "    cam_ = np.zeros((n_scans, 512, 512))\n",
    "    for cid in range(7):\n",
    "        if crop_info[cid] is None:\n",
    "            continue\n",
    "        xx1, xx2, yy1, yy2, zz1, zz2, _ = crop_info[cid]\n",
    "        cam_[zz1:zz2, xx1:xx2, yy1:yy2] += cam_to_intermediate_cam(grayscale_cam[cid*15:(cid+1)*15], zz2-zz1, zz2-zz1, xx2-xx1, yy2-yy1)\n",
    "\n",
    "    # Define the file path for saving\n",
    "    save_path = os.path.join(OUTPUT_FOLDER, f\"{cube_id}.npy\")\n",
    "\n",
    "    # Save the resized_cam as a .npy file\n",
    "    np.save(save_path, cam_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d10fbf-ad8d-4b39-bfa9-908d22111f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp39env",
   "language": "python",
   "name": "cp39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 228.336534,
   "end_time": "2022-10-27T04:03:17.394700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-27T03:59:29.058166",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
